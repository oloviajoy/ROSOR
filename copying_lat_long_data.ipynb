{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "33458a5e-acbc-410d-a837-b525ca9c1d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading df1 from: C:\\Users\\olivi\\Desktop\\PhaseOne IXM-100 photos\\.csv files\\oct_3_photos_in_flightlines.csv\n",
      "Loaded and processed 4277 valid records from C:\\Users\\olivi\\Desktop\\PhaseOne IXM-100 photos\\.csv files\\oct_3_photos_in_flightlines.csv (datetime adjusted by -4 hours).\n",
      "Saved processed df1 with adjusted datetimes to: oct_3_photos_with_parsed_datetime_adjusted.csv\n",
      "\n",
      "Loading df2 from: \\\\192.168.3.33\\nextcloud_2024\\Aurora 2025\\Deliverables\\Radiometric Data\\oct_3_\\assay_3478.csv\n",
      "Loaded 1784 valid records from \\\\192.168.3.33\\nextcloud_2024\\Aurora 2025\\Deliverables\\Radiometric Data\\oct_3_\\assay_3478.csv for processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olivi\\AppData\\Local\\Temp\\ipykernel_13828\\299981462.py:217: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '1043.484' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df2_processed.loc[index2, altitude_col_df2] = best_match_info['altitude']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 1784 records from df2. Found 1131 matches within Â±120 seconds.\n",
      "\n",
      "Updated data for df2 saved to oct_3_assay_updated_with_latlonalt_v2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import re # For regular expressions to parse filename\n",
    "\n",
    "# --- Configuration ---\n",
    "ASSUMED_YEAR = 2024 # General assumption, but file2 will try to use its own year\n",
    "ASSUMED_MONTH = 10\n",
    "ASSUMED_DAY = 3\n",
    "MATCHING_WINDOW_SECONDS = 120\n",
    "# NEW: Configuration for df1 datetime adjustment\n",
    "FILE1_DATETIME_SUBTRACT_HOURS = 4 # Hours to subtract from parsed df1 datetime\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def parse_test1_filename_to_datetime(filename_str, year, month, day):\n",
    "    # (Keep this function as it was, it returns the raw parsed datetime)\n",
    "    match = re.search(r'cap-(\\d{2}-\\d{2}-\\d{2}\\.\\d{3})\\.jpg', filename_str, re.IGNORECASE)\n",
    "    if not match:\n",
    "        match_alt = re.search(r'(\\d{2})[-_:]?(\\d{2})[-_:]?(\\d{2})[\\._](\\d{3})', filename_str)\n",
    "        if match_alt:\n",
    "            h, m, s, ms_str = match_alt.groups()\n",
    "            try:\n",
    "                return datetime(\n",
    "                    year, month, day,\n",
    "                    int(h), int(m), int(s), int(ms_str) * 1000\n",
    "                )\n",
    "            except ValueError as e:\n",
    "                # print(f\"Error parsing time from filename (alt pattern) '{filename_str}': {e}\")\n",
    "                return None\n",
    "        else:\n",
    "            # print(f\"Filename '{filename_str}' did not match expected patterns for time extraction.\")\n",
    "            return None\n",
    "\n",
    "    time_part_str = match.group(1)\n",
    "    try:\n",
    "        h, m, s_ms = time_part_str.split('-')\n",
    "        s, ms = s_ms.split('.')\n",
    "        return datetime(\n",
    "            year, month, day,\n",
    "            int(h), int(m), int(s), int(ms) * 1000\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        # print(f\"Error parsing time from filename '{filename_str}': {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_test2_datetime(date_str, time_str):\n",
    "    \"\"\"\n",
    "    Parses date (e.g., \"2024/10/03\" or \"15-Sep\") and time (e.g., \"07:17:11\") to a datetime object.\n",
    "    \"\"\"\n",
    "    date_str = str(date_str)\n",
    "    time_str = str(time_str)\n",
    "    full_datetime_str = f\"{date_str} {time_str}\"\n",
    "    try:\n",
    "        return datetime.strptime(full_datetime_str, \"%Y/%m/%d %H:%M:%S\")\n",
    "    except ValueError:\n",
    "        pass\n",
    "    try:\n",
    "        return datetime.strptime(full_datetime_str, \"%d-%b-%Y %H:%M:%S\")\n",
    "    except ValueError:\n",
    "        pass\n",
    "    if \" \" in date_str and \":\" in date_str:\n",
    "        try:\n",
    "            return datetime.strptime(date_str, \"%Y/%m/%d %H:%M:%S\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            return datetime.strptime(date_str, \"%d-%b-%Y %H:%M:%S\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "    print(f\"Error: Could not parse date/time from strings: Date='{date_str}', Time='{time_str}' with known formats.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Main Script ---\n",
    "def main():\n",
    "    # --- Configuration for File Paths ---\n",
    "    file1_path = r\"C:\\Users\\olivi\\Desktop\\PhaseOne IXM-100 photos\\.csv files\\oct_3_photos_in_flightlines.csv\"\n",
    "    file1_skiprows = 6\n",
    "    file1_assumed_year = 2024\n",
    "    file1_assumed_month = 10\n",
    "    file1_assumed_day = 3\n",
    "    file1_processed_output_path = \"oct_3_photos_with_parsed_datetime_adjusted.csv\" # Adjusted name\n",
    "\n",
    "    file2_path = r\"\\\\192.168.3.33\\nextcloud_2024\\Aurora 2025\\Deliverables\\Radiometric Data\\oct_3_\\assay_3478.csv\"\n",
    "\n",
    "    # 1. Load df1\n",
    "    print(f\"Loading df1 from: {file1_path}\")\n",
    "    try:\n",
    "        df1 = pd.read_csv(file1_path, skiprows=file1_skiprows)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file1_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file1_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    df1.columns = df1.columns.str.strip()\n",
    "    filename_col_df1 = '# Columns: Filename'\n",
    "    latitude_col_df1 = 'Vehicle Latitude'\n",
    "    longitude_col_df1 = 'Vehicle Longitude'\n",
    "    altitude_col_df1 = 'Vehicle Altitude'\n",
    "\n",
    "    # Basic column existence checks\n",
    "    required_cols_df1 = [filename_col_df1, latitude_col_df1, longitude_col_df1, altitude_col_df1]\n",
    "    for col in required_cols_df1:\n",
    "        if col not in df1.columns:\n",
    "            print(f\"Error: Column '{col}' not found in {file1_path}. Cols: {df1.columns.tolist()}\")\n",
    "            return\n",
    "\n",
    "    # Define the time adjustment for df1\n",
    "    df1_time_adjustment = timedelta(hours=FILE1_DATETIME_SUBTRACT_HOURS)\n",
    "\n",
    "    df1_processed = pd.DataFrame({\n",
    "        'original_filename': df1[filename_col_df1],\n",
    "        'datetime': df1[filename_col_df1].apply(\n",
    "            lambda x: (dt_obj - df1_time_adjustment)  # Apply adjustment here\n",
    "                      if (dt_obj := parse_test1_filename_to_datetime(x, file1_assumed_year, file1_assumed_month, file1_assumed_day)) is not None\n",
    "                      else None\n",
    "        ),\n",
    "        'latitude': df1[latitude_col_df1],\n",
    "        'longitude': df1[longitude_col_df1],\n",
    "        'altitude': df1[altitude_col_df1]\n",
    "    })\n",
    "    df1_processed.dropna(subset=['datetime'], inplace=True)\n",
    "\n",
    "    if df1_processed.empty:\n",
    "        print(f\"No valid records with parseable datetimes found in {file1_path} after processing.\")\n",
    "    else:\n",
    "        print(f\"Loaded and processed {len(df1_processed)} valid records from {file1_path} (datetime adjusted by -{FILE1_DATETIME_SUBTRACT_HOURS} hours).\")\n",
    "        try:\n",
    "            df1_processed.to_csv(file1_processed_output_path, index=False)\n",
    "            print(f\"Saved processed df1 with adjusted datetimes to: {file1_processed_output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving processed df1 to {file1_processed_output_path}: {e}\")\n",
    "        if df1_processed.empty:\n",
    "            return # Stop if no data to match against\n",
    "\n",
    "    # 2. Load df2\n",
    "    print(f\"\\nLoading df2 from: {file2_path}\")\n",
    "    try:\n",
    "        df2 = pd.read_csv(file2_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file2_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file2_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    df2.columns = df2.columns.str.strip()\n",
    "    date_col_df2 = 'Date'\n",
    "    time_col_df2 = 'Time'\n",
    "    id_col_df2 = 'Id'\n",
    "    latitude_col_df2 = 'Latitude'\n",
    "    longitude_col_df2 = 'Longitude'\n",
    "    altitude_col_df2 = 'Altitude' # Target column name\n",
    "\n",
    "    required_cols_df2 = [date_col_df2, time_col_df2] # Id is optional for processing logic\n",
    "    for col in required_cols_df2:\n",
    "        if col not in df2.columns:\n",
    "            print(f\"Error: Column '{col}' not found in {file2_path}. Cols: {df2.columns.tolist()}\")\n",
    "            return\n",
    "\n",
    "    df2['datetime'] = df2.apply(lambda row: parse_test2_datetime(row[date_col_df2], row[time_col_df2]), axis=1)\n",
    "    df2_valid_datetime_mask = df2['datetime'].notna()\n",
    "    df2_processed = df2[df2_valid_datetime_mask].copy()\n",
    "\n",
    "    if len(df2_processed) < len(df2):\n",
    "        print(f\"Warning: {len(df2) - len(df2_processed)} rows from {file2_path} had invalid date/time and were skipped.\")\n",
    "    if df2_processed.empty:\n",
    "        print(f\"No valid records with parseable datetimes found in {file2_path} after processing. Check date/time formats.\")\n",
    "        return\n",
    "    print(f\"Loaded {len(df2_processed)} valid records from {file2_path} for processing.\")\n",
    "\n",
    "    # 3. Match and Update\n",
    "    cols_to_populate = {\n",
    "        latitude_col_df2: 0.0,\n",
    "        longitude_col_df2: 0.0,\n",
    "        altitude_col_df2: 0.0,\n",
    "        'Time_Diff_To_Match_Sec': pd.NA\n",
    "    }\n",
    "\n",
    "    for col, default_val in cols_to_populate.items():\n",
    "        if col not in df2_processed.columns:\n",
    "            df2_processed[col] = default_val\n",
    "        # Ensure numeric for lat, lon, alt if they exist but might be non-numeric\n",
    "        if col not in ['Time_Diff_To_Match_Sec']:\n",
    "             df2_processed[col] = pd.to_numeric(df2_processed[col], errors='coerce').fillna(default_val)\n",
    "\n",
    "\n",
    "    matches_found = 0\n",
    "    for index2, row2 in df2_processed.iterrows():\n",
    "        time2 = row2['datetime']\n",
    "        if pd.isna(time2): continue\n",
    "\n",
    "        best_match_info = None\n",
    "        min_time_diff_seconds = MATCHING_WINDOW_SECONDS + 1\n",
    "\n",
    "        for _, row1 in df1_processed.iterrows():\n",
    "            time1 = row1['datetime']\n",
    "            # if pd.isna(time1): continue # Should not happen due to dropna on df1_processed\n",
    "\n",
    "            time_diff = abs(time1 - time2)\n",
    "            current_diff_seconds = time_diff.total_seconds()\n",
    "\n",
    "            if current_diff_seconds <= MATCHING_WINDOW_SECONDS:\n",
    "                if current_diff_seconds < min_time_diff_seconds:\n",
    "                    min_time_diff_seconds = current_diff_seconds\n",
    "                    best_match_info = {\n",
    "                        'latitude': row1['latitude'],\n",
    "                        'longitude': row1['longitude'],\n",
    "                        'altitude': row1['altitude'],\n",
    "                        'time_diff_seconds': current_diff_seconds\n",
    "                    }\n",
    "\n",
    "        if best_match_info:\n",
    "            df2_processed.loc[index2, latitude_col_df2] = best_match_info['latitude']\n",
    "            df2_processed.loc[index2, longitude_col_df2] = best_match_info['longitude']\n",
    "            df2_processed.loc[index2, altitude_col_df2] = best_match_info['altitude']\n",
    "            df2_processed.loc[index2, 'Time_Diff_To_Match_Sec'] = best_match_info['time_diff_seconds']\n",
    "            matches_found += 1\n",
    "\n",
    "    print(f\"\\nProcessed {len(df2_processed)} records from df2. Found {matches_found} matches within Â±{MATCHING_WINDOW_SECONDS} seconds.\")\n",
    "\n",
    "    # Update the original df2 with the processed rows\n",
    "    # Ensure columns to be updated exist in original df2, adding them if necessary with default values for non-matched rows\n",
    "    for col_to_update in [latitude_col_df2, longitude_col_df2, altitude_col_df2, 'Time_Diff_To_Match_Sec']:\n",
    "        if col_to_update not in df2.columns:\n",
    "            if col_to_update == 'Time_Diff_To_Match_Sec':\n",
    "                df2[col_to_update] = pd.NA\n",
    "            else: # lat, lon, alt\n",
    "                df2[col_to_update] = 0.0 # Default for rows in df2 not in df2_processed\n",
    "\n",
    "    # Update df2 with values from df2_processed.\n",
    "    # This transfers matched lat/lon/alt and Time_Diff_To_Match_Sec.\n",
    "    # Rows in df2 that were filtered out (not in df2_processed) will retain their original values\n",
    "    # or the defaults set in the loop above if the columns were newly added to df2.\n",
    "    update_cols = [latitude_col_df2, longitude_col_df2, altitude_col_df2, 'Time_Diff_To_Match_Sec']\n",
    "    df2.update(df2_processed[update_cols])\n",
    "\n",
    "\n",
    "    # 4. Save the Result\n",
    "    output_filename = 'oct_3_assay_updated_with_latlonalt_v2.csv' # Adjusted output name\n",
    "\n",
    "    cols_to_drop_from_output = ['datetime']\n",
    "    df2_output = df2.copy()\n",
    "    for col in cols_to_drop_from_output:\n",
    "        if col in df2_output.columns:\n",
    "            df2_output = df2_output.drop(columns=[col])\n",
    "\n",
    "    try:\n",
    "        df2_output.to_csv(output_filename, index=False)\n",
    "        print(f\"\\nUpdated data for df2 saved to {output_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving output file for df2: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f1d408-4af9-4821-923b-286e1af345d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29302655-cd0b-4cc0-bf0b-5287077fef16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
